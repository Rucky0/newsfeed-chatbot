{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'BUCKET_NAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# set up the bucket name\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBUCKET_NAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# set up the url\u001b[39;00m\n\u001b[1;32m     23\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.mfn.se/nyheter/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BUCKET_NAME'"
     ]
    }
   ],
   "source": [
    "# help me make a scraper for MFN news data from the website using AWS Lambda\n",
    "# https://www.mfn.se/nyheter/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "# set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# set up the s3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# set up the bucket name\n",
    "bucket_name = os.environ['BUCKET_NAME']\n",
    "\n",
    "# set up the url\n",
    "url = 'https://www.mfn.se/nyheter/'\n",
    "\n",
    "# set up the headers\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# set up the function\n",
    "def lambda_handler(event, context):\n",
    "    # get the html content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # get the news data\n",
    "    news_data = []\n",
    "    for news in soup.find_all('div', class_='news-item'):\n",
    "        news_data.append({\n",
    "            'title': news.find('a').text,\n",
    "            'link': news.find('a')['href'],\n",
    "            'date': news.find('span', class_='date').text\n",
    "        })\n",
    "    # save the news data to s3\n",
    "    file_name = 'news_data.json'\n",
    "    s3.put_object(Bucket=bucket_name, Key=file_name, Body=json.dumps(news_data))\n",
    "    # log the success\n",
    "    logger.info('News data saved to s3')\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('News data saved to s3')\n",
    "    }\n",
    "\n",
    "# test the function\n",
    "lambda_handler(None, None)\n",
    "\n",
    "# check the s3 bucket\n",
    "response = s3.get_object(Bucket=bucket_name, Key='news_data.json')\n",
    "news_data = json.loads(response['Body'].read())\n",
    "print(news_data)\n",
    "\n",
    "# check the logs\n",
    "# go to the cloudwatch logs\n",
    "# click on the log group\n",
    "# click on the log stream\n",
    "# click on the log event\n",
    "# check the log messages\n",
    "\n",
    "'''\n",
    "\n",
    "## Step 5: Schedule the Lambda Function\n",
    "\n",
    "Now that we have the Lambda function, we can schedule it to run at regular intervals. This will allow us to scrape the news data from the website automatically.\n",
    "\n",
    "Here's how you can schedule the Lambda function:\n",
    "\n",
    "1. Go to the AWS Management Console and navigate to the Lambda service.\n",
    "2. Click on the Lambda function that you created earlier.\n",
    "3. Click on the \"Add trigger\" button.\n",
    "4. Select \"CloudWatch Events/EventBridge\" as the trigger type.\n",
    "5. Click on the \"Create a new rule\" button.\n",
    "6. Enter a name for the rule, such as \"ScrapeNewsData\".\n",
    "7. Set the schedule expression to the desired interval, e.g., `rate(1 day)`.\n",
    "8. Click on the \"Add\" button to add the trigger.\n",
    "9. Click on the \"Save\" button to save the changes.\n",
    "\n",
    "Now the Lambda function will run at the specified interval and scrape the news data from the website.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have learned how to scrape news data from a website using AWS Lambda. We have set up a Lambda function that scrapes the news data from the website and saves it to an S3 bucket. We have also scheduled the Lambda function to run at regular intervals using CloudWatch Events/EventBridge.\n",
    "\n",
    "This approach allows us to automate the process of scraping news data from the website and store it in a centralized location for further analysis. It can be useful for monitoring news updates, tracking trends, and generating insights from the data.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in /Users/bawar/Library/Python/3.9/lib/python/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/bawar/Library/Python/3.9/lib/python/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/bawar/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
